@article{TPO0,
	title        = {TelePhysicalOperation: Remote Robot Control Based on a Virtual “Marionette” Type Interaction Interface},
	author       = {Torielli, Davide and Muratore, Luca and Laurenzi, Arturo and Tsagarakis, Nikos},
	year         = 2022,
	journal      = {{IEEE} Robotics and Automation Letters},
	volume       = 7,
	number       = 2,
	pages        = {2479-2486},
	doi          = {10.1109/LRA.2022.3144792},
	url = {https://ieeexplore.ieee.org/document/9696192},
	abbr         = {RA-L},
	abstract = {Teleoperation permits to control robots from a safe distance while performing tasks in a remote environment. Kinematic differences between the input device and the remotely controlled manipulator or the existence of redundancy in the remote robot may pose challenges in moving intuitively the remote robot as desired by the human operator. Motivated by the above challenges, this work introduces TelePhysicalOperation, a novel teloperation concept, which relies on a virtual physical interaction interface between the human operator and the remote robot in a manner that is equivalent to a “Marionette” based interaction interface. With the proposed approach, the user can virtually “interact” with the remote robot, through the application of virtual forces, which are generated by the operator tracking system and can be then selectively applied to any body part of the remote robot along its kinematic chain. This leads to the remote robot generating motions that comply with the applied virtual forces, thanks to the underlying control architecture. The proposed method permits to command the robot from a distance by exploring the intuitiveness of the “Marionette” based physical interaction with the robot in a virtual/remote manner. The details of the proposed approach are introduced and its effectiveness is demonstrated through a number of experimental trials executed on the CENTAURO, a hybrid leg-wheel platform with an anthropomorphic upper body.},
	bibtex_show={true},
	html = {https://ieeexplore.ieee.org/document/9696192},
	pdf = {tpo.pdf},
	poster = {ICRA22_POSTER.pdf},
	video = {https://www.youtube.com/embed/dkBmbTyO_GQ},
	altmetric = {true},
	dimensions = {true},
	selected   = {true},
}
@inproceedings{TPO2,
	title        = {Manipulability-Aware Shared Locomanipulation Motion Generation for Teleoperation of Mobile Manipulators},
	author       = {Torielli, Davide and Muratore, Luca and Tsagarakis, Nikos},
	year         = 2022,
	booktitle    = {{IEEE/RSJ} International Conference on Intelligent Robots and Systems},
	volume       = {},
	number       = {},
	pages        = {6205-6212},
	doi          = {10.1109/IROS47612.2022.9982220},
	abbr         = {IROS},
	abstract = {The teleoperation of mobile manipulators may pose significant challenges, demanding complex interfaces and causing a substantial burden to the human operator due to the need to switch continuously from the manipulation of the arm to the control of the mobile platform. Hence, several works have considered to exploit shared control techniques to overcome this issue and, in general, to facilitate the task execution. This work proposes a manipulability-aware shared locoma-nipulation motion generation method to facilitate the execution of telemanipulation tasks with mobile manipulators. The method uses the manipulability level of the end-effector to control the generation of the mobile base and manipulator motions, facilitating their simultaneous control by the operator while executing telemanipulation tasks. Therefore, the operator can exclusively control the end -effector, while the underlying ar-chitecture generates the mobile platform commands depending on the end-effector manipulability level. The effectiveness of this approach is demonstrated with a number of experiments in which the CENTAURO robot, a hybrid leg-wheel platform with an anthropomorphic upper body, is teleoperated to execute a set of telemanipulation tasks.},
	bibtex_show={true},
	html = {https://ieeexplore.ieee.org/document/9982220},
	poster = {IROS22_POSTER.pdf},
	video = {https://www.youtube.com/embed/7YqfVn8XvNk},
	altmetric = {true},
	dimensions = {true},
}
@inproceedings{TPO3,
	title        = {A Shared Telemanipulation Interface to Facilitate Bimanual Grasping and Transportation of Objects of Unknown Mass},
    author       = {Torielli, Davide and Muratore, Luca and De Luca, Alessio and Tsagarakis, Nikos},
 	year         = 2022,
	booktitle    = {{IEEE-RAS} International Conference on Humanoid Robots},
	volume       = {},
	number       = {},
	pages = {738-745},
	doi          = {10.1109/Humanoids53995.2022.10000094},
	abbr         = {HUMANOIDS},
	abstract = {The teleoperation of robots with human-like capabilities may pose significant challenges to the human operator due to the kinematic complexity and redundancy of these robots. Bimanual telemanipulation represents such a challenging task that requires precise coordination of the two arms to perform a stable bimanual grasp on an object and eventually transport the object while maintaining the grasp. In this work, we present a shared control telemanipulation interface to facilitate the bimanual grasping and transportation of objects of unknown mass. With the proposed method, the robot is able to transport the object maintaining autonomously a sufficient amount of grasping force while accepting commands from the operator to reach the desired location. As humans do, it is not necessary to know the weight of the object in advance; instead, the robot estimates it during the lifting phase. On the basis of the estimated weight, the required amount of grasping force is computed. During object transportation, the robot autonomously regulates the grasping forces in a shared control fashion, allowing the operator to seamlessly command only the trajectories of the object. The proposed method has been implemented and validated on the CENTAURO robot, a quadrupedal platform with a humanoid dual arm upper body, performing experiment where objects of different weights and dimensions must be picked up and transported.},
	bibtex_show={true},
	html = {https://ieeexplore.ieee.org/document/10000094},
	video = {https://www.youtube.com/embed/7s1n0weq5To},
	altmetric = {true},
	dimensions = {true},
}

@inproceedings{TPOIRIM,
	title        = {TelePhysicalOperation: a Shared Control Architecture for Intuitive and Smart Teleoperation of Complex Mobile Manipulators},
	author       = {Torielli, Davide and Muratore, Luca and Tsagarakis, Nikos},
	year         = 2022,
	booktitle    = {Italian Conference in Robotics and Intelligent Machines},
	volume       = {},
	number       = {},
	pages        = {255--259},
	url          = {https://doi.org/10.5281/zenodo.7797398},
	doi          = {10.5281/zenodo.7797398},
	abbr         = {IRIM},
	bibtex_show={true},
	html = {https://zenodo.org/records/7797398},
	poster = {IRIM22_POSTER.pdf},
	altmetric = {true},
	dimensions = {true},
}

@inproceedings{TPO4,
	title        = {Wearable Haptics for a Marionette-inspired Teleoperation of Highly Redundant Robotic Systems},
	author       = {Torielli, Davide and Franco, Leonardo and Pozzi, Maria and Muratore, Luca and Malvezzi, Monica and Tsagarakis, Nikos and Prattichizzo, Domenico},
	year         = 2024,
	booktitle    = {{IEEE} International Conference on Robotics and Automation},
	volume       = {},
	number       = {},
  	pages	     = {15670-15676},
	html         = {https://ieeexplore.ieee.org/abstract/document/10610788},
	url          = {https://ieeexplore.ieee.org/abstract/document/10610788},
    doi          = {10.1109/ICRA57147.2024.10610788},
	abbr         = {ICRA},
	abstract     = {The teleoperation of complex, kinematically redundant robots with loco-manipulation capabilities represents a challenge for human operators, who have to learn how to operate the many degrees of freedom of the robot to accomplish a desired task. In this context, developing an easy-to-learn and easy-to-use human-robot interface is paramount. Recent works introduced a novel teleoperation concept, which relies on a virtual physical interaction interface between the human operator and the remote robot equivalent to a "Marionette" control, but whose feedback was limited to only visual feedback on the human side. In this paper, we propose extending the "Marionette" interface by adding a wearable haptic interface to cope with the limitations given by the previous work. Leveraging the additional haptic feedback modality, the human operator gains full sensorimotor control over the robot, and the awareness about the robot's response and interactions with the environment is greatly improved. We evaluated the proposed interface and the related teleoperation framework with naive users, assessing the teleoperation performance and the user experience with and without haptic feedback. The conducted experiments consisted in a loco-manipulation mission with the CENTAURO robot, a hybrid leg-wheel quadruped with a humanoid dual-arm upper body.},
	bibtex_show  = {true},
	video = {https://www.youtube.com/embed/QLU2ZrU0HjQ?},
	poster = {ICRA24_POSTER.pdf},
	altmetric = {true},
	dimensions = {true},
}

@article{LaserJournal,
	title = {An intuitive tele-collaboration interface exploring laser-based interaction and behavior trees},
	author       = {Torielli, Davide and Muratore, Luca and Tsagarakis, Nikos},
	journal = {Robotics and Autonomous Systems},
	volume = {193},
	pages = {105054},
	year = {2025},
	issn = {0921-8890},
	doi = {https://doi.org/10.1016/j.robot.2025.105054},
	url = {https://www.sciencedirect.com/science/article/pii/S092188902500140X},
	keywords = {Human-robot interface, Human-centered robotics, Visual servoing, Motion planning},
	abstract = {The recent advancements in the development of robotic systems that offer advanced loco-manipulation capabilities have opened new opportunities in the employment of such platforms in various domains. However, despite the increased range of offered capabilities, the collaboration with these robotic platforms to execute tasks through common human–robot interaction interfaces is still an open challenge. In this article, we present a novel human–robot interaction interface that permits to intuitively command and control the manipulation and the locomotion abilities of the robot by exploring a visual servoing guidance method realized with a laser emitter device. By pointing the laser to locations and objects in the environment where the robot is operating, the operator is able to command even highly articulated robots intuitively and efficiently. The detection of the laser projection is performed by a neural network that provides robust and real-time tracking of laser spot. Combined with the responsiveness of the laser detection, a Behavior Trees-based motion planner is employed to reactively select and generate the autonomous robot motions to reach the indicated target. This combination allows the operator to communicate goal locations and paths to follow without requiring prior knowledge of the system, and without worrying about the generation of the potential complex loco-manipulation robot actions. The effectiveness of the proposed interface is demonstrated with the CENTAURO robot, a hybrid leg-wheel platform with an anthropomorphic upper body, exploiting its abilities to accomplish a number of locomotion and manipulation tasks.}
	abbr         = {RAS},
	bibtex_show={true},
	html = {https://www.sciencedirect.com/science/article/pii/S092188902500140X},
	video = {https://youtu.be/zin8LRSNwWo},
	altmetric = {true},
	dimensions = {true},
	selected   = {true},
}

@article{LaserRAL,
	author     = {Torielli, Davide and Bertoni, Liana and Muratore, Luca and Tsagarakis, Nikos},
	title = {A Laser-Guided Interaction Interface for Providing Effective Robot Assistance to People With Upper Limbs Impairments},
	year         = 2024,
	journal      = {{IEEE} Robotics and Automation Letters},
	volume       = {9},
	number       = {9},
	pages        = {7653-7660},
	doi          = {10.1109/LRA.2024.3430709},
	keywords     = {Robots;Lasers;Task analysis;Keyboards;Magnetic heads;Surface emitting lasers;Grippers;Human-robot collaboration;physically assistive devices;visual servoing},
	url = {https://ieeexplore.ieee.org/document/10602529},
	abbr         = {RA-L},
	abstract = {Robotics has shown significant potential in assisting people with disabilities to enhance their independence and involvement in daily activities. Indeed, a societal long-term impact is expected in home-care assistance with the deployment of intelligent robotic interfaces. This work presents a human-robot interface developed to help people with upper limbs impairments, such as those affected by stroke injuries, in activities of everyday life. The proposed interface leverages on a visual servoing guidance component, which utilizes an inexpensive but effective laser emitter device. By projecting the laser on a surface within the workspace of the robot, the user is able to guide the robotic manipulator to desired locations, to reach, grasp and manipulate objects. Considering the targeted users, the laser emitter is worn on the head, enabling to intuitively control the robot motions with head movements that point the laser in the environment, which projection is detected with a neural network based perception module. The interface implements two control modalities: the first allows the user to select specific locations directly, commanding the robot to reach those points; the second employs a paper keyboard with buttons that can be virtually pressed by pointing the laser at them. These buttons enable a more direct control of the Cartesian velocity of the end-effector and provides additional functionalities such as commanding the action of the gripper. The proposed interface is evaluated in a series of manipulation tasks involving a 6DOF assistive robot manipulator equipped with 1DOF beak-like gripper. The two interface modalities are combined to successfully accomplish tasks requiring bimanual capacity that is usually affected in people with upper limbs impairments.},
	bibtex_show={true},
	html = {https://ieeexplore.ieee.org/document/10602529},
	pdf = {laserRal.pdf},
	video = {https://youtu.be/WyWfgpezwRs},
	altmetric = {true},
	dimensions = {true},
	selected   = {true},
}

@articleB{RoseePaperLiana,
	title        = {Towards a Generic Grasp Planning Pipeline using End-Effector Specific Primitive Grasping Actions},
	author       = {Liana Bertoni and Davide Torielli and Yifang Zhang and Nikos G. Tsagarakis and Luca Muratore},
	year         = 2021,
	journal      = {{IEEE} International Conference on Advanced Robotics},
	volume       = {},
	number       = {},
	pages        = {},
	doi          = {10.1109/ICAR53236.2021.9659402},
	abbr         = {ICAR},
	bibtex_show={true},
	html = {https://ieeexplore.ieee.org/document/9659402},
	altmetric = {true},
	dimensions = {true},
}

@inproceedings{RoseePaper,
	title        = {Towards an Open-Source Hardware Agnostic Framework for Robotic End-Effectors Control},
	author       = {Davide Torielli and Liana Bertoni and Nikos G. Tsagarakis and Luca Muratore},
	year         = 2021,
	journal      = {{IEEE} International Conference on Advanced Robotics},
	volume       = {},
	number       = {},
	pages        = {},
	doi          = {10.1109/ICAR53236.2021.9659331},
	abbr         = {ICAR},
	bibtex_show={true},
	abstract={Nowadays a wide range of industrial grippers are available on the market and usually their integration to robotics automation systems relies on dedicated software modules and interfaces specific for each gripper. During the past two decades, more sophisticated end-effector modules that target to provide additional functionality including dexterous manipulation skills as well as sensing capabilities have been developed. The integration of these new devices is usually not trivial, requiring the development of brand new, tailor-made software modules and interfaces, which is a time consuming and certainly not efficient activity. To address the above issue and facilitate the quick integration and validation of the new end-effectors, we developed the ROS End-Effector open-source framework, which provides a software infrastructure capable to accommodate a range of robotic end-effectors of different hardware characteristics (number of fingers, actuators, sensing modules and communication protocols) and capabilities (with different manipulation skills, such as grasping, pinching, or independent finger dexterity) effectively facilitating their integration through the development of hardware agnostic software modules, simulation tools and application programming interfaces (APIs). A key feature of the ROS End-Effector framework is that rather than controlling each end-effector in a different and customized way, following specific protocols and instructions data fields, it masks the physical hardware differences and limitations (e.g., kinematics and dynamic model, actuator, sensor, update frequency, etc.) and permits to command the end-effector using a set of high level grasping primitives. The framework capabilities and flexibility in supporting different robotics end-effectors are demonstrated both in a kinematic/dynamic simulation and in real hardware experiments.},
	html = {https://ieeexplore.ieee.org/document/9659331},
	code = {https://github.com/ADVRHumanoids/ROSEndEffector},
	altmetric = {true},
	dimensions = {true},
}

@article{ROSEE,
	doi = {10.1007/s10846-023-01911-5},
	url = {https://doi.org/10.1007/s10846-023-01911-5},
	year = {2023},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {108},
	number = {4},
	author = {Davide Torielli and Liana Bertoni and Fabio Fusaro and Nikos Tsagarakis and Luca Muratore},
	title = {{ROS} End-Effector: A Hardware-Agnostic Software and Control Framework for Robotic End-Effectors},
	journal = {Journal of Intelligent \& Robotic Systems},
	selected   = {true},
	abbr         = {JIRS},
	abstract = {In recent years, several robotic end-effectors have been developed and made available in the market. Nevertheless, their adoption in industrial context is still limited due to a burdensome integration, which strongly relies on customized software modules specific for each end-effector. Indeed, to enable the functionalities of these end-effectors, dedicated interfaces must be developed to consider the different end-effector characteristics, like finger kinematics, actuation systems, and communication protocols. To face the challenges described above, we present ROS End-Effector, an open-source framework capable of accommodating a wide range of robotic end-effectors of different grasping capabilities (grasping, pinching, or independent finger dexterity) and hardware characteristics. The ROS End-Effector framework, rather than controlling each end-effector in a different and customized way, allows to mask the physical hardware differences and permits to control the end-effector using a set of high-level grasping primitives automatically extracted. By leveraging on hardware agnostic software modules including hardware abstraction layer (HAL), application programming interfaces (APIs), simulation tools and graphical user interfaces (GUIs), ROS End-Effector effectively facilitates the integration of diverse end-effector devices. The proposed framework capabilities in supporting different robotics end-effectors are demonstrated in both simulated and real hardware experiments using a variety of end-effectors with diverse characteristics, ranging from under-actuated grippers to anthropomorphic robotic hands. Finally, from the user perspective, the manuscript provides a set of examples about the use of the framework showing its flexibility in integrating a new end-effector module.},
	bibtex_show={true},
	html = {https://link.springer.com/article/10.1007/s10846-023-01911-5},
	pdf = {rosee.pdf},
	code = {https://github.com/ADVRHumanoids/ROSEndEffector},
	video = {https://www.youtube.com/embed/X0qpSsFQg1M},
	altmetric = {true},
	dimensions = {true},
	selected   = {true},
}

@articleB{Muratore2023,
	AUTHOR = {Muratore, Luca and Laurenzi, Arturo and De Luca, Alessio and Bertoni, Liana and Torielli, Davide and Baccelliere, Lorenzo and Del Bianco, Edoardo and Tsagarakis, Nikos G.},
	TITLE = {A Unified Multimodal Interface for the RELAX High-Payload Collaborative Robot},
	JOURNAL = {Sensors},
	VOLUME = {23},
	YEAR = {2023},
	NUMBER = {18},
	ARTICLE-NUMBER = {7735},
	url = {https://www.mdpi.com/1424-8220/23/18/7735},
	ISSN = {1424-8220},
	DOI = {10.3390/s23187735},
	abbr         = {SENSORS},
	abstract = {This manuscript introduces a mobile cobot equipped with a custom-designed high payload arm called RELAX combined with a novel unified multimodal interface that facilitates Human–Robot Collaboration (HRC) tasks requiring high-level interaction forces on a real-world scale. The proposed multimodal framework is capable of combining physical interaction, Ultra Wide-Band (UWB) radio sensing, a Graphical User Interface (GUI), verbal control, and gesture interfaces, combining the benefits of all these different modalities and allowing humans to accurately and efficiently command the RELAX mobile cobot and collaborate with it. The effectiveness of the multimodal interface is evaluated in scenarios where the operator guides RELAX to reach designated locations in the environment while avoiding obstacles and performing high-payload transportation tasks, again in a collaborative fashion. The results demonstrate that a human co-worker can productively complete complex missions and command the RELAX mobile cobot using the proposed multimodal interaction framework.},
	bibtex_show={true},
	html = {https://www.mdpi.com/1424-8220/23/18/7735},
	altmetric = {true},
	dimensions = {true},
}

@articleB{Dagana,
	author={Del Bianco, Edoardo and Torielli, Davide and Rollo, Federico and Gasperini, Damiano and Laurenzi, Arturo and Baccelliere, Lorenzo and Muratore, Luca and Roveri, Marco and Tsagarakis, Nikos G.},
	booktitle    = {{IEEE-RAS} International Conference on Humanoid Robots},
	title={A High-Force Gripper with Embedded Multimodal Sensing for Powerful and Perception Driven Grasping}, 
	year={2024},
	volume={},
	number={},
	pages={149-156},
	keywords={Multimodal sensors;Force;Robot vision systems;Pose estimation;Humanoid robots;Grasping;Thermal force;Grippers;Robots;Payloads},
	doi={10.1109/Humanoids58906.2024.10769951},
	abbr         = {HUMANOIDS},
	abstract = {Modern humanoid robots have shown their promising potential for executing various tasks involving the grasping and manipulation of objects using their end-effectors. Nevertheless, in the most of the cases, the grasping and manipulation actions involve low to moderate payload and interaction forces. This is due to limitations often presented by the end-effectors, which can not match their arm-reachable payload, and hence limit the payload that can be grasped and manipulated. In addition, grippers usually do not embed adequate perception in their hardware, and grasping actions are mainly driven by perception sensors installed in the rest of the robot body, frequently affected by occlusions due to the arm motions during the execution of the grasping and manipulation tasks. To address the above, we developed a modular high grasping force gripper equipped with embedded multi-modal perception functionalities. The proposed gripper can generate a grasping force of 110 N in a compact implementation. The high grasping force capability is combined with embedded multi-modal sensing, which includes an eye-in-hand camera, a Time-of-Flight (ToF) distance sensor, an Inertial Measurement Unit (IMU) and an omnidirectional microphone, permitting the implementation of perception-driven grasping functionalities. We extensively evaluated the grasping force capacity of the gripper by introducing novel payload evaluation metrics that are a function of the robot arm’s dynamic motion and gripper thermal states. We also evaluated the embedded multi-modal sensing by performing perception-guided enhanced grasping operations.},
	bibtex_show={true},
	html = {https://ieeexplore.ieee.org/abstract/document/10769951},
	video = {https://www.youtube.com/embed/QhYZGOF-i-I},
	altmetric = {true},
	dimensions = {true},
}

